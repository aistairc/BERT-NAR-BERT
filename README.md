# BERT2BERT
Pre-trained sequence-to-sequence (S2S) models have recently attracted great attention for their wide scope of applications. However, these models are often limited to short texts in generation due to the slow autoregressive decoding. Furthermore, such pre-training is independent of the widely successful pre-trained BERT and related models, especially for decoding, due to the structural differences in BERT-related models and S2S models. The decoders are often limited to specific autoregressive language models, such as GPT-2. Thus, the encoder and decoder need to deal with inconsistent tokenization. To resolve these problems and train a fast and scalable S2S model, we aim at building a non-autoregressive S2S model that is flexible enough to employ BERTs as itâ€™s backbone for not only the encoder but also the decoder. In this poster, as a first step to validate this aim, we propose to pre-train BERT models using the non-autoregressive approach, which we call BERT-to-BERT. We evaluate the pre-trained BERT-to-BERT model using the standard GLUE tasks. Experimental results show that the pre-training strategy is effective for BERT. It shows how our model with different settings consistently performs better than the original BERT model and the Optimus model.
